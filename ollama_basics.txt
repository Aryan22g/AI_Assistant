Ollama: next-gen ai model deployment framework

How does it work? --> local AI model runner, handling execution of LLMs, etc.
Steps: Install Ollama--> Download model-->run an ai model--> integrate with applications.

Popular models:
1.	Llama3
2.	Mistral 7B (lightweight Transformer) --> fast, cost-efficient AI applications
3.	CodeLlama (AI for code generation)
4.	Gemini1.5
5.	Gpt-4 turbo
6.	Mixtral
7.	Phi-2 
8.	Whisper (speech-to-Text AI)
9.	starCoder (AI powered Code assistant)
10.	Med-PalM2 (AI for medical applications)

Install Ollama & setup
1.	Download ollama from website
2.	Download python (3.12 or above)  (python -v)
3.	Create venv:
  a.	python -m venv <env_name>
  b.	to activate:
    i.	windows: ( <env_name>\Scripts\activate )
    ii.	mac/linux: ( source <env_name>/bin/activate )
4.	install python packages:
  a.	pip install requests openai fastapi streamlit
5.	running your first AI model;
  a.	download an ai model (here, Mistral): ollama pull mistral
  b.	run a basic ai chat session: ollama run mistral
  c.	running a model programmatically with python
